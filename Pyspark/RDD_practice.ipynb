{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### @Author: Sankar\n",
    "#### @Date: 2021-05-05 09:13:25\n",
    "#### @Last Modified by: Sankar\n",
    "#### @Last Modified time: 2021-05-05 11:45:09\n",
    "#### @Title:RDD Practice and Word Count Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD -> 8\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "       \"pyspark and spark\"]\n",
    ")\n",
    "counts = words.count()\n",
    "print(\"Number of elements in RDD -> %i\" % (counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('JK', 22),\n",
       " ('V', 24),\n",
       " ('Jimin', 24),\n",
       " ('RM', 25),\n",
       " ('J-Hope', 25),\n",
       " ('Suga', 26),\n",
       " ('Jin', 27)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD = sc.parallelize([('JK', 22), ('V', 24), ('Jimin',24), ('RM', 25), ('J-Hope', 25), ('Suga', 26), ('Jin', 27)])\n",
    "myRDD.take(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A wonderful king is Hadoop.',\n",
       " 'The elephant plays well with Sqoop.',\n",
       " 'But what helps him to thrive',\n",
       " 'Are Impala, and Hive,']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc = SparkContext(\"local\",\"Word count example\")\n",
    "RDD_Hadoop = sc.textFile(\"hdfs://localhost:9000/usr/hadoop/data/data.txt\")\n",
    "RDD_Hadoop.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = RDD_Hadoop.flatMap(lambda line: line.split(\" \"))\n",
    "wordCounts = words.map(lambda word: (word, 1)).reduceByKey(lambda a,b:a+b)\n",
    "wordCounts.saveAsTextFile(\"hdfs://localhost:9000/usr/hadoop/data/output5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (2, 9)), ('b', (3, 7))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize([('a',2),('b',3)])\n",
    "b = sc.parallelize([('a',9),('b',7),('c',10)])\n",
    "# Performing Join operation on the RDDs\n",
    "c = a.join(b)\n",
    "c.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12497500"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an RDD and performing a lambda function to get the sum of elements in the RDD\n",
    "num_rdd = sc.parallelize(range(1,5000))\n",
    "num_rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 4), ('c', 2), ('a', 12), ('d', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using ReduceByKey transformation to reduce the data\n",
    "data_keydata_key = sc.parallelize([('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1),('d', 3)],4)\n",
    "data_keydata_key.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sorting the data based on a key\n",
    "test = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(test).sortByKey(True, 1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing Set Operations\n",
    "rdd_a = sc.parallelize([1,2,3,4])\n",
    "rdd_b = sc.parallelize([3,4,5,6])\n",
    "\n",
    "rdd_a.intersection(rdd_b).collect()\n",
    "\n",
    "rdd_a.subtract(rdd_b).collect()\n",
    "\n",
    "rdd_a.cartesian(rdd_b).collect()\n",
    "\n",
    "rdd_a.union(rdd_b).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
